{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits identification and classification using MNIST dataset\n",
    "\n",
    "## Classify hand written digits 0 through 9\n",
    "\n",
    "In this exercise we will explore Classification - identifying and then labeling objects in to their classes.\n",
    "We will be using **Tensorflow** as the framework to do this and exploring **neural networks** and **deep learning!**\n",
    "\n",
    "### This exercise in ML is like the \"Hello World\" for programming ! There are other frameworks beside Tensorflow such as mxnet that can be used to solve this. \n",
    "\n",
    "For more information check out Tensorflow tutorials https://www.tensorflow.org/tutorials/\n",
    "\n",
    "The data we will be using comes from MNIST (Modified National Institute of Standards and Technology) dataset that contains images of handwritten digits (by US Censor staff and highschool students) https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "An example extract is below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/mnist_examples.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "# just like our regression exercise using sklearn, tensorflow also has built-in datasets - in this case the MNIST set\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data\n",
    "\n",
    "### Please note that we used the tensorflow built-in functions to import the MNIST data above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images and labels into mnist.test (10K images+labels) and mnist.train (60K images+labels)\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at a written digit in the MNIST set\n",
    "ex_number = mnist.train.images[37]  # change the index here to see different images\n",
    "ex_number = ex_number.reshape(28, 28)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111) \n",
    "plt.imshow(ex_number, cmap='gray')  \n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(orientation='vertical')\n",
    "plt.show() \n",
    "print (mnist.train.labels[37])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic One-Layer NN -- in pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> Convert the image into data\n",
    "<font size = 4> We flatten the image into an 784-dimensional array of greyscale values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/mnist-input2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> To begin we'll design a 1-layer neural network with 10 output neurons since we want to classify digits into 10 classes (0 to 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/full_one_layer_example.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5> There are a handful of activation functions that are commonly used\n",
    "* <font size = 4> softmax\n",
    "* <font size = 4> sigmoid\n",
    "* <font size = 4> ReLu, leaky ReLU\n",
    "* <font size = 4> tanh, LeCun's tanh\n",
    "* <font size = 4> others\n",
    "\n",
    "\n",
    "\n",
    "<font size = 5> We use the cross entropy to measure the error\n",
    "\n",
    "### $$ \\text{cross entropy} = -\\sum Y_i'\\log(Y_i) $$\n",
    "\n",
    "\n",
    "<font size = 3>\n",
    "Cross entropy - wikipedia - https://en.wikipedia.org/wiki/Cross_entropy \n",
    "\n",
    "What is Cross Entropy  ? https://stackoverflow.com/questions/41990250/what-is-cross-entropy\n",
    "\n",
    "Loss functions - http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Regressions \n",
    "\n",
    "{from Tensorflow tutorials https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners}\n",
    "\n",
    "\n",
    "We know that every image in MNIST is of a handwritten digit between zero and nine. So there are only ten possible things that a given image can be. We want to be able to look at an image and give the probabilities for it being each digit. For example, our model might look at a picture of a nine and be 80% sure it's a nine, but give a 5% chance to it being an eight (because of the top loop) and a bit of probability to all the others because it isn't 100% sure.\n",
    "\n",
    "This is a classic case where a softmax regression is a natural, simple model. If you want to assign probabilities to an object being one of several different things, softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1. Even later on, when we train more sophisticated models, the final step will be a layer of softmax.\n",
    "\n",
    "A softmax regression has two steps: first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic One-Layer Neural Network -- at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Rather than run through one example as a time, we'll take advantage of linear algebra and run through batches of 100 at a time. These are called **mini-batches**. One pass through the entire set of training examples is called an **epoch**.\n",
    "\n",
    "<font size = 4> It is possible to compute your gradient one element (stochastic gradient descent) at a time but using mini-batches is more efficient computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/one_layer_gif.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> And our activation function takes the following form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/one_layer_output.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> The error is computed just as before, using the cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/one_layer_crossentropy.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Why work with \"mini-batches\" of 100 images and labels ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> It's possible to compute your gradient on just one example image at a time (\"stochastic gradient descent\"). Doing so on 100 examples gives a gradient that better represents the constraints imposed by different example images and is therefore likely to converge towards the solution faster. \n",
    "\n",
    "Can also adjust the size of the mini-batch. There is another, more technical reason: working with batches also means working with larger matrices and these are usually easier to optimise on GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Why use cross entropy instead of Mean Square Error ?\n",
    "<font size = 4> You can use both but average cross entropy is considered better. Loosely, compared to average cross entropy, MSE gives too much weight to incorrect outputs. Also MSE a good measure for Regression models and Cross Entropy is for Classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic One-Layer Neural Network -- Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Set up place holders (a tensorflow concept) for the computational graph. \n",
    "\n",
    "These are parameters that will be filled with actual data from the actual training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])  # see explanation below\n",
    "\n",
    "# weights W[784, 10]   784=28*28\n",
    "W = tf.Variable(tf.zeros([784, 10]))  # zeroing the matrix \n",
    "\n",
    "# biases b[10]\n",
    "b = tf.Variable(tf.zeros([10]))  # zeroing the vector\n",
    "\n",
    "init = tf.global_variables_initializer() # initializing everything !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Note that the shape of the tensor **X** holding the training images is [None, 28, 28, 1]. Here is what those arguments mean:\n",
    "\n",
    "    'None': this dimension will be the number of images in the mini-batch. It will be known at training time.\n",
    "    \n",
    "    '28, 28, 1': our images are 28x28 pixels x 1 value per pixel (grayscale). The last number would be 3 for color images and is not really necessary here.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Note also that we have initialized the weights and biases to be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "Y = tf.nn.softmax(tf.matmul(tf.reshape(X, [-1, 784]), W) + b)\n",
    "\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Here the **tf.reshape(X, [-1, 784])** command flattens the image pixels to single array. The model then computes the softmax, given by the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\textbf{Y} = softmax(\\textbf{X}.\\textbf{W} + \\textbf{b}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> We'll also make a place to hold the correct answers from our labeled data. Recall, that to compute the error of our measurement, we use the cross-entropy. Why this and not the Mean Square Error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$ \\text{cross entropy} = -\\sum Y_i'\\log(Y_i) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function in code\n",
    "cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> This takes the log of each element and multiplies the tensors element by element. \n",
    "\n",
    "We'll also keep track of the accuracy from our test set of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of correct answers found in batch, between 0 (worst) and 1 (best)\n",
    "is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> **tf.equal** is a boolean that returns the truth value of the actual (Y) and our prediction element-wise (Y_). \n",
    "\n",
    "***accuracy*** first casts **is_correct** as a float value type, then computes the mean of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the model using Gradient Descent\n",
    "\n",
    "\n",
    "Gradient Descent - All you need to know https://hackernoon.com/gradient-descent-aynk-7cbe95a778da "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Next, we come to the gradient descent part. Here we tell the algorithm how to optimize to minimize the cost. \n",
    "\n",
    "You have a choice in selecting an optimizer (there are many available). TensorFlow will compute the partial derivatives of the loss function relatively to all the weights and all the biases. \n",
    "\n",
    "The gradient is then used to update the weights and biases. We'll tell the optimizer to use 0.003 as the **learning rate**. Recall, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/learning_rate1.png\" alt=\"Drawing\" style=\"width: 300px;\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select an optimizer and define the training step\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.003)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. What if the learing rate were smaller? What if the learning rate were larger? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/learning_rate_DL.png\" alt=\"Drawing\" style=\"width: 550px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> By now the computational graph structure is complete. Next, we need to feed actual data into the graph and compute something. We start up a **Session** which will keep track of the computational graph and update values of the weights through each mini-batch. \n",
    "    \n",
    " ### So far we have only 'set up' the Tensorflow compute graph. Now with the \"sess.run\" call we actually execute the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1000):\n",
    "    # load batch of images and correct answers\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    train_data={X: batch_X, Y_: batch_Y}\n",
    "\n",
    "    # train\n",
    "    sess.run(train_step, feed_dict=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Once we've run through each mini-batch in our training data, we want to see how good our basic one-layer trained neural network does at identifying hand written digits. We can compute extract the accuracy and cross_entropy from our neural network with the following command. Here we are feeding it the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success ?\n",
    "a_train,c_train = sess.run([accuracy, cross_entropy], feed_dict=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The accuracy for this model on training data is %.3f.\" %  a_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Is this a fair assessment of our accuracy and final cross-entropy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success on test data ?\n",
    "test_data={X: mnist.test.images, Y_: mnist.test.labels}\n",
    "a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The accuracy for this model on the test data is %.3f.\" %  a_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented as a python program! $$\\text{mnist_softmax.py}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding layers -- in pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Now we'll try to improve our model by adding more layers to our neural network. In pictures, this looks something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/five_layers.png\" alt=\"Drawing\" style=\"width: 650px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> There are five layers. \n",
    "\n",
    "    The first layer has 200 nodes\n",
    "    The second layer has 100 nodes\n",
    "    The third layer has 60 nodes\n",
    "    The fourth layer has 30 nodes\n",
    "    The fifth layer has 10 nodes\n",
    "    \n",
    "Note also that we use the **sigmoid** function as the activation function in the intermediate layers. And keep **softmax** as the final layer because these are the nodes representing the digits 0 through 9 and softmax does a good job at classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> The algorithm will be the same as before \n",
    "\n",
    "    Training digits and labels on a mini-batch\n",
    "    => compute loss function, cross entropy with current weights\n",
    "    => compute gradient (partial derivatives) of loss w.r.t. weights and biases \n",
    "    => move in direction of steepest descent according to learning rate\n",
    "    => update weights and biases \n",
    "    => repeat with next mini-batch of training images and labels\n",
    "    \n",
    "Only now to compute our loss function requires these intermediate steps.\\\\\n",
    "Another thing to keep in mind is that we should initialize our weights to non-zero values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding layers -- in code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Much of our code will remain the same. We only need to add additional lines for the extra layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X: 28x28 grayscale images, the first dimension (None) will index the images in the mini-batch\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# correct answers will go here\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# five layers and their number of neurons (the last layer has 10 softmax neurons)\n",
    "L = 200\n",
    "M = 100\n",
    "N = 60\n",
    "O = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll initialize our weights in each layer randomly by using a truncated normal distribution of values between -0.2 and 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. Why use a truncated normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> It is important to initialise weights with random values to keep the optimizer from getting stuck in its initial position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first layer\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.zeros([L]))\n",
    "\n",
    "#second layer using output from first layer \n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([M]))\n",
    "\n",
    "#third layer using output from second layer\n",
    "W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([N]))\n",
    "\n",
    "#fourth layer using output from third layer\n",
    "W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "B4 = tf.Variable(tf.zeros([O]))\n",
    "\n",
    "#fifth layer using output from fourth layer\n",
    "W5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Then, we create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model\n",
    "XX = tf.reshape(X, [-1, 784])                  #XX is X with flattened out the pixels\n",
    "\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
    "Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + B3)\n",
    "Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + B4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Then, compute the cross entropy, accuracy and set the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function in code\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# compute the error\n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability\n",
    "# problems with log(0) which is NaN\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_sum(cross_entropy)\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# training step, learning rate = 0.003\n",
    "#combined the previous two lines into one\n",
    "train_step = tf.train.GradientDescentOptimizer(0.003).minimize(cross_entropy)\n",
    "\n",
    "# initialize the Tensorflow session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# train the NN\n",
    "for i in range(10000):\n",
    "    # load batch of images and correct answers\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    train_data={X: batch_X, Y_: batch_Y}\n",
    "\n",
    "    # train\n",
    "    sess.run(train_step, feed_dict=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Now let's look at the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# success on test data ?\n",
    "test_data={X: mnist.test.images, Y_: mnist.test.labels}\n",
    "a_test,c_test = sess.run([accuracy, cross_entropy], feed_dict=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The accuracy for this multi-layer model on the test data is %.3f.\" %  a_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In conclusion\n",
    "        *We loaded the MNIST digits dataset and checked it out\n",
    "        * We tested a single layer neural network and calculated its accuracy\n",
    "        * We tested a multi layer neural network and showed improvement in accuracy\n",
    "        * We saw Tensorflow functionality (although briefly)\n",
    "        \n",
    "### Some questions you may want to ponder upon:\n",
    "\n",
    "#### Q: How would changing the number of learning rate affect the accuracy of our NN?\n",
    "#### Q: How would changing the number of iterations affect the accuracy of our NN?\n",
    "#### Q: How would changing the number of layers affect the accuracy of our NN?\n",
    "#### Q: How would changing the number of nodes within a layer affect the accuracy of our NN?\n",
    "      Q: What's the purpose of life ? :) Who am I ? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\text{mnist_fivelayers.py}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced topics follow! \n",
    "\n",
    "The topics below will need extra reading so please tread carefully! There is a lot of information on the Net. The Tensorflow tutorials will be a huge help. There are several free resources on Deep Learning. \n",
    "\n",
    "The state of the art continues to change so enjoy!\n",
    "\n",
    "\n",
    "## Tuning a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> The deeper a NN gets, the harder it can be for it to converge to the solution. Here are some simple fixes to improve your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> The sigmoid function can be problematic because it squashes values between 0 and 1. This can cause neuron gradients to vanish and slow convergence. Instead of sigmoid we can use ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/relu.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Simply swap **tf.nn.sigmoid** in the code above with **tf.nn.relu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a differnet optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> In high dimensional spaces with tens of thousands of weights and biases, saddle points can occur which will trap and confuse your optimizer. The saddle points are a problem because the gradient is zero but it isn't because you're at a minimum (or maximum). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/saddle.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Swap **tf.train.GradientDescentOptimiser** with a **tf.train.AdamOptimizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Randomizing Initialisations\n",
    "\n",
    "<font size = 4> When working with ReLU it is recommended to choose initial weights that are small **and positive**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Why do you need to have non-zero values for ReLU?\n",
    "\n",
    "<font size = 4> So that neurons can operate in the non-zero range of ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Try the following code, making all the initial biases 0.1. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "W1 = tf.Variable(tf.truncated_normal([784, L], stddev=0.1))  # 784 = 28 * 28\n",
    "B1 = tf.Variable(tf.ones([L])/10)                            # 0.1 = tf.ones([L])/10\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([M])/10)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([N])/10)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([N, O], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([O])/10)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([O, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Increasing the number of iterations will improve your accuracy but the results can be noisy and the test accuracy can vary wildly. Try it!\n",
    "\n",
    "One way to avoid this is to start the learning decay rate fast but then decrease slowly to 0.0001 or smaller. \n",
    "\n",
    "Add the following code to add learning rate decay."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# learning rate decay\n",
    "max_learning_rate = 0.003\n",
    "min_learning_rate = 0.0001\n",
    "decay_speed = 2000.0 # 0.003-0.0001-2000=>0.9826 done in 5000 iterations\n",
    "learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Try plotting the training and test curves to see the test curve become more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> A central problem in machine learning is to create a model that performs well on the training set as well as new data. Methods to do this are collectively referred to as **regularization** techniques. There are many regularization techniques in deep learning. A popular one is called **dropout**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/dropout.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4>Different neurons will be dropped at each iteration. When testing the performance of your network of course you put all the neurons back (pkeep=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Luckily TensorFlow has a dropout capability built-in. You can adjust the code as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The model, with dropout at each layer\n",
    "XX = tf.reshape(X, [-1, 28*28])\n",
    "\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + B1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "Ylogits = tf.matmul(Y4d, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\text{mnist_five_layers_relu_lrdecay_dropout.py}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> In a layer of a convolutional network, one \"neuron\" does a weighted sum of the pixels just above it, across a small region of the image only.\n",
    "\n",
    "Imagine we have a color picture and the color at a pixel is given by the values of RBG at that pixel. By sliding a 4x4 window across the image we can obtain weighted average of each window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./pics/con_neural_net.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> The TensorFlow syntax for that is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/cnn_tfsyntax.png\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Can add as many convolutional layers as you like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/CNN_many_layers.png\" alt=\"Drawing\" style=\"width: 650px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Coding a Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> We'll build a convolutional neural net with 3 convolutional layers, a fully connected layer and our original softmax layer. \n",
    "\n",
    "We first need to initialize the weights in the weighted averages..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three convolutional layers with their channel counts, and a\n",
    "# fully connected layer (tha last layer has 10 softmax neurons)\n",
    "K = 4  # first convolutional layer output depth\n",
    "L = 8  # second convolutional layer output depth\n",
    "M = 12  # third convolutional layer\n",
    "N = 200  # fully connected layer\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.1))  # 5x5 patch, 1 input channel, K output channels\n",
    "B1 = tf.Variable(tf.ones([K])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([L])/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([M])/10)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([N])/10)\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.ones([10])/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> Next we need to implement the convolutional layers. To do this we can use **tf.nn.conv2d**. We still need to add in the bias then apply the activation function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#first convolutional layer, output is 28x28\n",
    "stride = 1  \n",
    "Y1_cnv = tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME')\n",
    "Y1 = tf.nn.relu(Y1_cnv + B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 4> The padding strategy that works here is to copy pixels from the sides of the image. All digits are on a uniform background so this just extends the background and should not add any unwanted shapes.\n",
    "'SAME' is the type of padding algorithm to use\n",
    "\n",
    "Now we code the rest of the layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#second convolutional layer, output is 14x14\n",
    "stride = 2  \n",
    "Y2_cnv = tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') \n",
    "Y2 = tf.nn.relu(Y2_cnv + B2)\n",
    "\n",
    "#third convolutional layer, output is 7x7\n",
    "stride = 2\n",
    "Y3_cnv = tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME')\n",
    "Y3 = tf.nn.relu(Y3_cnv + B3)\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "\n",
    "Ylogits = tf.matmul(Y4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $$\\text{mnist_convolutional.py}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
